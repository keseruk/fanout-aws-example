AWSTemplateFormatVersion: 2010-09-09
Parameters:
  S3BucketName:
    Type: String
    Default: The bucket name from the parent stack.
  SQSResizeQueueArn:
    Type: String
    Description: The resize SQS queue ARN from the parent stack.
  SQSProcessorQueueArn:
    Type: String
    Description: The processor SQS queue ARN from the parent stack.
Resources:
  LambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - sts:AssumeRole
      Policies:
        - PolicyName: allowLambdaLogs
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - logs:*
                Resource: arn:aws:logs:*:*:*
        - PolicyName: allowSqs
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - sqs:ReceiveMessage
                  - sqs:DeleteMessage
                  - sqs:GetQueueAttributes
                  - sqs:ChangeMessageVisibility
                Resource:
                  - !Ref SQSResizeQueueArn
                  - !Ref SQSProcessorQueueArn
        - PolicyName: allowRekognition
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - rekognition:DetectLabels
                Resource: !Join ["", ["*"]]
        - PolicyName: allowS3
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              Effect: "Allow"
              Action:
                - "s3:GetObject"
                - "s3:PutObject"
                - "s3:ListBucket"
                - "s3:PutObjectTagging"
              Resource:
                - !Sub "arn:aws:s3:::${S3BucketName}"
                - !Sub "arn:aws:s3:::${S3BucketName}/*"

  ProcessorLambda:
    Type: AWS::Lambda::Function
    Properties:
      Code:
#        S3Bucket: cf-templates-khpdujqevcyg-eu-central-1
#        S3Key: mom/lambda/mom_lambda.zip
        ZipFile: |
          import boto3
          import json
          import logging

          logger = logging.getLogger()
          logger.setLevel(logging.INFO)


          def copy_with_metadata(bucket: str, object_key: str, labels: dict):
              """
              Makes a copy of the image with labels added as metadata
              For example, cat123.jpg is copied to processed/cat123/cat123_original.jpg

              :param bucket: S3 bucket name
              :param object_key: S3 object key
              :param labels: Key-value pairs to be added as object meta
              """
              tags = "&".join([f"{key}={value}" for key, value in labels.items()])

              s3 = boto3.resource("s3")
              filename = object_key.split("/")[-1][:-4]
              s3.Object(bucket, f"processed/{filename}/{filename}_original.jpg").copy_from(
                  CopySource={'Bucket': bucket, 'Key': object_key}, Tagging=tags, TaggingDirective="REPLACE")


          def get_labels(bucket: str, object_key: str, num_labels: int = 5) -> dict:
              """
              Get labels from the Amazon Rekognition service.

              :param bucket: The S3 bucket name
              :param object_key: The S3 object name
              :param num_labels: The number of labels to be returned
              :return: Returns the list of 
              """
              client = boto3.client("rekognition", region_name="eu-central-1")
              logging.info(f"Detecting labels of {object_key} in bucket: {bucket}")

              response = client.detect_labels(Image={"S3Object": {"Bucket": bucket, "Name": object_key}},
                                              MinConfidence=50, MaxLabels=num_labels)

              labels = {}
              for label in response["Labels"]:
                  labels[label.get("Name")] = str(label.get("Confidence"))

              return labels


          def handler(event, context):
              """
              Lambda handler function.

              :param event: The event from the SQS service
              :param context: Event context
              """
              if "Records" in event and len(event["Records"]) > 0:

                  payload = event["Records"][0].get("body")
                  json_payload = json.loads(payload)
                  bucket = json_payload["Records"][0]["s3"]["bucket"]["name"]
                  object_key = json_payload["Records"][0]["s3"]["object"]["key"]

                  if "processed" in object_key:
                      return

                  try:
                      labels = get_labels(bucket, object_key)
                      copy_with_metadata(bucket, object_key, labels)
                  except Exception as e:
                      logging.exception(f"Failed to process s3://{bucket}/{object_key}")
                      raise e

      Handler: index.handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Runtime: python3.9
      Timeout: 60
      MemorySize: 512

  ProcessorLambdaEventSourceMapping:
    Type: AWS::Lambda::EventSourceMapping
    Properties:
      BatchSize: 1
      Enabled: true
      EventSourceArn: !Ref SQSProcessorQueueArn
      FunctionName: !GetAtt ProcessorLambda.Arn

  ThumbnailCreatorLambda:
    Type: AWS::Lambda::Function
    Properties:
      Code:
#        S3Bucket: cf-templates-khpdujqevcyg-eu-central-1
#        S3Key: mom/lambda/mom_lambda.zip
        ZipFile: |
          import boto3
          import json
          import logging
          from typing import Optional
          from io import BytesIO
          from PIL import Image

          logger = logging.getLogger()
          logger.setLevel(logging.INFO)

          s3 = boto3.resource('s3')


          def generate_thumbnail(bucket: str, key: str) -> Optional[BytesIO]:
              """
              Creates a thumbnail using the Python image library and returns it as a byte stream.

              :param bucket: The bucket that holds the source image.
              :param key: The object key in the bucket.
              :return: Returns a byte stream of the thumbnail.
              """
              client = boto3.client('s3')
              size = 200, 200
              stream = BytesIO()
              s3_object = client.get_object(Bucket=bucket, Key=key)['Body'].read()
              image = Image.open(BytesIO(s3_object))
              image.thumbnail(size, Image.ANTIALIAS)  # PIL keeps the aspect ratio
              image.save(stream, format=image.format)
              stream.seek(0)
              return stream


          def upload_file(byte_stream: BytesIO, bucket: str, key: str) -> str:
              """
              Uploads a file from the given byte stream into S3.

              :param file_bytes: Image byte stream.
              :param bucket: The target bucket
              :param key: The object key in the bucket.
              :return: Returns the fully qualified S3 object path.
              """
              client = boto3.client('s3')
              res = client.put_object(Body=byte_stream, Bucket=bucket, Key=key)
              logging.info(f"Image uploaded to s3://{bucket}/{key}")
              return f"s3://{bucket}/{key}"


          def handler(event, context):
              """
              Lambda handler function. Creates a thumbnail and uploads it to /processed/[FILENAME]/thumbnail_[FILENAME].jpg

              Batch processing is currently not supported.
              Make sure that the BatchSize is set to 1 in the function's SQS EventSourceMapping.

              :param event: The event from the SQS service
              :param context: Event context
              """
              if "Records" in event and len(event["Records"]) > 0:
                  # process the first record only
                  payload = event["Records"][0].get("body")
                  json_payload = json.loads(payload)
                  bucket = json_payload["Records"][0]["s3"]["bucket"]["name"]
                  object_key = json_payload["Records"][0]["s3"]["object"]["key"]

                  # the S3 notification filter should ensure that already processed objects won't be processed again
                  # however I prefer adding a check to avoid endless processing
                  if "processed" in object_key:
                      return

                  try:
                      logging.info(f"Processing s3://{bucket}/{object_key}")
                      thumbnail_bytes = generate_thumbnail(bucket, object_key)
                      filename = object_key.split("/")[-1][:-4]
                      if (thumbnail_bytes):
                          upload_file(thumbnail_bytes, bucket, f"processed/{filename}/thumbnail_{filename}.jpg")

                  except Exception as e:
                      logging.exception(f"Failed to process s3://{bucket}/{object_key}")
                      raise e

      Layers:
        - arn:aws:lambda:eu-central-1:770693421928:layer:Klayers-p39-pillow:1
      Handler: index.handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Runtime: python3.9
      Timeout: 60
      MemorySize: 512

  ResizeLambdaEventSourceMapping:
    Type: AWS::Lambda::EventSourceMapping
    Properties:
      BatchSize: 1
      Enabled: true
      EventSourceArn: !Ref SQSResizeQueueArn
      FunctionName: !GetAtt ThumbnailCreatorLambda.Arn
